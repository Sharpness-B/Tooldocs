[{"question":"# Q1. Partitioner controls the partitioning of what data?","options":["final keys","final values","intermediate keys","intermediate values"],"anwserIndex":2,"answerText":"intermediate keys"},{"question":"# Q2. SQL Windowing functions are implemented in Hive using which keywords?","options":["UNION DISTINCT, RANK","OVER, RANK","OVER, EXCEPT","UNION DISTINCT, RANK"],"anwserIndex":1,"answerText":"OVER, RANK"},{"question":"# Q3. Rather than adding a Secondary Sort to a slow Reduce job, it is Hadoop best practice to perform which optimization?","options":["Add a partitioned shuffle to the Map job.","Add a partitioned shuffle to the Reduce job.","Break the Reduce job into multiple, chained Reduce jobs.","Break the Reduce job into multiple, chained Map jobs."],"anwserIndex":1,"answerText":"Add a partitioned shuffle to the Reduce job."},{"question":"# Q4. Hadoop Auth enforces authentication on protected resources. Once authentication has been established, it sets what type of authenticating cookie?","options":["encrypted HTTP","unsigned HTTP","compressed HTTP","signed HTTP"],"anwserIndex":3,"answerText":"signed HTTP"},{"question":"# Q5. MapReduce jobs can be written in which language?","options":["Java or Python","SQL only","SQL or Java","Python or SQL"],"anwserIndex":0,"answerText":"Java or Python"},{"question":"# Q6. To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?","options":["Reducer","Combiner","Mapper","Counter"],"anwserIndex":1,"answerText":"Combiner"},{"question":"# Q7. To verify job status, look for the value `**\\_**` in the `**\\_**`.","options":["SUCCEEDED; syslog","SUCCEEDED; stdout","DONE; syslog","DONE; stdout"],"anwserIndex":1,"answerText":"SUCCEEDED; stdout"},{"question":"# Q8. Which line of code implements a Reducer method in MapReduce 2.0?","options":["public void reduce(Text key, Iterator<IntWritable> values, Context context){…}","public static void reduce(Text key, IntWritable[] values, Context context){…}","public static void reduce(Text key, Iterator<IntWritable> values, Context context){…}","public void reduce(Text key, IntWritable[] values, Context context){…}"],"anwserIndex":0,"answerText":"public void reduce(Text key, Iterator<IntWritable> values, Context context){…}"},{"question":"# Q9. To get the total number of mapped input records in a map job task, you should review the value of which counter?","options":["FileInputFormatCounter","FileSystemCounter","JobCounter","TaskCounter (NOT SURE)"],"anwserIndex":3,"answerText":"TaskCounter (NOT SURE)"},{"question":"# Q10. Hadoop Core supports which CAP capabilities?","options":["A, P","C, A","C, P","C, A, P"],"anwserIndex":0,"answerText":"A, P"},{"question":"# Q11. What are the primary phases of a Reducer?","options":["combine, map, and reduce","shuffle, sort, and reduce","reduce, sort, and combine","map, sort, and combine"],"anwserIndex":1,"answerText":"shuffle, sort, and reduce"},{"question":"# Q12. To set up Hadoop workflow with synchronization of data between jobs that process tasks both on disk and in memory, use the **\\_** service, which is **\\_**.","options":["Oozie; open source","Oozie; commercial software","Zookeeper; commercial software","Zookeeper; open source"],"anwserIndex":3,"answerText":"Zookeeper; open source"},{"question":"# Q13. For high availability, use multiple nodes of which type?","options":["data","name","memory","worker"],"anwserIndex":1,"answerText":"name"},{"question":"# Q14. DataNode supports which type of drives?","options":["hot swappable","cold swappable","warm swappable","non-swappable"],"anwserIndex":0,"answerText":"hot swappable"},{"question":"# Q15. Which method is used to implement Spark jobs?","options":["on disk of all workers","on disk of the master node","in memory of the master node","in memory of all workers"],"anwserIndex":3,"answerText":"in memory of all workers"},{"question":"# Q16. In a MapReduce job, where does the map() function run?","options":["on the reducer nodes of the cluster","on the data nodes of the cluster (NOT SURE)","on the master node of the cluster","on every node of the cluster"],"anwserIndex":1,"answerText":"on the data nodes of the cluster (NOT SURE)"},{"question":"# Q17. To reference a master file for lookups during Mapping, what type of cache should be used?","options":["distributed cache","local cache","partitioned cache","cluster cache"],"anwserIndex":0,"answerText":"distributed cache"},{"question":"# Q18. Skip bad records provides an option where a certain set of bad input records can be skipped when processing what type of data?","options":["cache inputs","reducer inputs","intermediate values","map inputs"],"anwserIndex":3,"answerText":"map inputs"}]